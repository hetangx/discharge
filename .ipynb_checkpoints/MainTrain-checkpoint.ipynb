{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597023784740",
   "display_name": "Python 3.8.1 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原型阶段：方法暴露，不要封装，先完成基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "# 一些函数\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = r\"D:\\Codes\\keyan\\peidian\\train_list.txt\"\n",
    "mms, stand = tools.SK_scaler(tools.LinkData(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdDataset(Data.Dataset): # 创建自己的类：MyDataset,这个类是继承的torch.utils.data.Dataset\n",
    "    def __init__(self, root, csvfile): # 初始化需要传入的参数\n",
    "        super(PdDataset,self).__init__()\n",
    "        fh = open(root + csvfile, 'r') #按照传入的路径和txt名，打开文本读取内容\n",
    "        csvs = [] # 创建空列表\n",
    "        for line in fh: # 按行循环txt文本中的内容\n",
    "            line = line.rstrip() # 删除本行string字符串末尾的指定字符\n",
    "            words = line.split() # 通过指定分隔符对字符串进行切片\n",
    "            csvs.append((words[0],words[1])) # 把txt里的内容读入csv列表保存，[0]为文件路径，[1]是label\n",
    "        \n",
    "        self.csvs = csvs\n",
    "        \n",
    "    def __getitem__(self, index): #按照索引读取每个元素的具体内容\n",
    "        fn, label = self.csvs[index] #fn和label分别获得csvs[index]也即是刚才每行中word[0]和word[1]的信息\n",
    "        csv = pd.read_csv(fn, header=None)\n",
    "        csv = tools.DataTrans(csv.values.astype(float), mms, stand)\n",
    "        csv = torch.from_numpy(csv)\n",
    "        csv = csv.permute(1, 0).float()\n",
    "        return csv, label # return返回哪些内容，在训练时循环读取每个batch时就能获得哪些内容\n",
    " \n",
    "    def __len__(self): #返回数据集的长度，也就是多少个文件，要和loader的长度作区分\n",
    "        return len(self.csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"d:\\\\Codes\\\\keyan\\\\peidian\\\\\"\n",
    "#根据自己定义的PdDataset创建数据集\n",
    "train_data=PdDataset(root, \"train_list.txt\")\n",
    "test_data=PdDataset(root, \"test_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name = ['corona_discharge', 'float_discharge', 'interference', 'internal_discharge', 'surface_discharge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 100, 10)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "        self.conv2 = nn.Conv1d(100, 100, 10)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "        self.conv3 = nn.Conv1d(100, 160, 10)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(160*10, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = x.view(-1, 160*10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # 权值初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNNNet()\n",
    "net = net.cuda()\n",
    "net.initialize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数与优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, dampening=0.1)    # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)     # 设置学习率下降策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training: Epoch[001/020] Iteration[010/100] Loss: 0.6139 Acc:62.50%\nTraining: Epoch[001/020] Iteration[020/100] Loss: 0.5989 Acc:63.75%\nTraining: Epoch[001/020] Iteration[030/100] Loss: 0.5792 Acc:62.50%\nTraining: Epoch[001/020] Iteration[040/100] Loss: 0.6074 Acc:61.25%\nTraining: Epoch[001/020] Iteration[050/100] Loss: 0.7081 Acc:61.50%\nTraining: Epoch[001/020] Iteration[060/100] Loss: 0.5638 Acc:63.33%\nTraining: Epoch[001/020] Iteration[070/100] Loss: 0.6295 Acc:62.86%\nTraining: Epoch[001/020] Iteration[080/100] Loss: 0.8507 Acc:60.31%\nTraining: Epoch[001/020] Iteration[090/100] Loss: 0.7326 Acc:59.44%\nTraining: Epoch[001/020] Iteration[100/100] Loss: 0.6461 Acc:60.00%\nValid set Accuracy:60.00%\nTraining: Epoch[002/020] Iteration[010/100] Loss: 0.7583 Acc:62.50%\nTraining: Epoch[002/020] Iteration[020/100] Loss: 0.4558 Acc:70.00%\nTraining: Epoch[002/020] Iteration[030/100] Loss: 0.8414 Acc:59.17%\nTraining: Epoch[002/020] Iteration[040/100] Loss: 0.6537 Acc:58.75%\nTraining: Epoch[002/020] Iteration[050/100] Loss: 0.6396 Acc:61.00%\nTraining: Epoch[002/020] Iteration[060/100] Loss: 0.7427 Acc:60.00%\nTraining: Epoch[002/020] Iteration[070/100] Loss: 0.6060 Acc:61.07%\nTraining: Epoch[002/020] Iteration[080/100] Loss: 0.5886 Acc:60.62%\nTraining: Epoch[002/020] Iteration[090/100] Loss: 0.6286 Acc:60.00%\nTraining: Epoch[002/020] Iteration[100/100] Loss: 0.6177 Acc:60.00%\nTraining: Epoch[003/020] Iteration[010/100] Loss: 0.6066 Acc:67.50%\nTraining: Epoch[003/020] Iteration[020/100] Loss: 0.7802 Acc:61.25%\nTraining: Epoch[003/020] Iteration[030/100] Loss: 0.6525 Acc:59.17%\nTraining: Epoch[003/020] Iteration[040/100] Loss: 0.6923 Acc:58.13%\nTraining: Epoch[003/020] Iteration[050/100] Loss: 0.5937 Acc:59.00%\nTraining: Epoch[003/020] Iteration[060/100] Loss: 0.6000 Acc:58.33%\nTraining: Epoch[003/020] Iteration[070/100] Loss: 0.6562 Acc:59.64%\nTraining: Epoch[003/020] Iteration[080/100] Loss: 0.6781 Acc:59.69%\nTraining: Epoch[003/020] Iteration[090/100] Loss: 0.6782 Acc:59.17%\nTraining: Epoch[003/020] Iteration[100/100] Loss: 0.5915 Acc:59.50%\nValid set Accuracy:60.00%\nTraining: Epoch[004/020] Iteration[010/100] Loss: 0.6402 Acc:60.00%\nTraining: Epoch[004/020] Iteration[020/100] Loss: 0.5821 Acc:61.25%\nTraining: Epoch[004/020] Iteration[030/100] Loss: 0.7316 Acc:61.67%\nTraining: Epoch[004/020] Iteration[040/100] Loss: 0.6936 Acc:62.50%\nTraining: Epoch[004/020] Iteration[050/100] Loss: 0.6953 Acc:60.00%\nTraining: Epoch[004/020] Iteration[060/100] Loss: 0.6494 Acc:59.17%\nTraining: Epoch[004/020] Iteration[070/100] Loss: 0.6236 Acc:59.64%\nTraining: Epoch[004/020] Iteration[080/100] Loss: 0.6029 Acc:59.38%\nTraining: Epoch[004/020] Iteration[090/100] Loss: 0.6469 Acc:59.72%\nTraining: Epoch[004/020] Iteration[100/100] Loss: 0.6589 Acc:59.50%\nTraining: Epoch[005/020] Iteration[010/100] Loss: 0.8091 Acc:57.50%\nTraining: Epoch[005/020] Iteration[020/100] Loss: 0.7449 Acc:58.75%\nTraining: Epoch[005/020] Iteration[030/100] Loss: 0.7219 Acc:56.67%\nTraining: Epoch[005/020] Iteration[040/100] Loss: 0.6588 Acc:55.00%\nTraining: Epoch[005/020] Iteration[050/100] Loss: 0.4812 Acc:59.50%\nTraining: Epoch[005/020] Iteration[060/100] Loss: 0.7246 Acc:58.75%\nTraining: Epoch[005/020] Iteration[070/100] Loss: 0.7110 Acc:57.50%\nTraining: Epoch[005/020] Iteration[080/100] Loss: 0.5072 Acc:59.38%\nTraining: Epoch[005/020] Iteration[090/100] Loss: 0.4800 Acc:60.83%\nTraining: Epoch[005/020] Iteration[100/100] Loss: 0.6811 Acc:60.00%\nValid set Accuracy:60.00%\nTraining: Epoch[006/020] Iteration[010/100] Loss: 0.7059 Acc:50.00%\nTraining: Epoch[006/020] Iteration[020/100] Loss: 0.5759 Acc:55.00%\nTraining: Epoch[006/020] Iteration[030/100] Loss: 0.6139 Acc:57.50%\nTraining: Epoch[006/020] Iteration[040/100] Loss: 0.5769 Acc:60.00%\nTraining: Epoch[006/020] Iteration[050/100] Loss: 0.6328 Acc:62.00%\nTraining: Epoch[006/020] Iteration[060/100] Loss: 0.6336 Acc:61.25%\nTraining: Epoch[006/020] Iteration[070/100] Loss: 0.6439 Acc:60.36%\nTraining: Epoch[006/020] Iteration[080/100] Loss: 0.7535 Acc:59.69%\nTraining: Epoch[006/020] Iteration[090/100] Loss: 0.7089 Acc:59.44%\nTraining: Epoch[006/020] Iteration[100/100] Loss: 0.6725 Acc:59.00%\nTraining: Epoch[007/020] Iteration[010/100] Loss: 0.6746 Acc:60.00%\nTraining: Epoch[007/020] Iteration[020/100] Loss: 0.7132 Acc:56.25%\nTraining: Epoch[007/020] Iteration[030/100] Loss: 0.6910 Acc:55.83%\nTraining: Epoch[007/020] Iteration[040/100] Loss: 0.7093 Acc:55.00%\nTraining: Epoch[007/020] Iteration[050/100] Loss: 0.6249 Acc:56.00%\nTraining: Epoch[007/020] Iteration[060/100] Loss: 0.6837 Acc:57.50%\nTraining: Epoch[007/020] Iteration[070/100] Loss: 0.6420 Acc:57.14%\nTraining: Epoch[007/020] Iteration[080/100] Loss: 0.6378 Acc:58.13%\nTraining: Epoch[007/020] Iteration[090/100] Loss: 0.5474 Acc:59.17%\nTraining: Epoch[007/020] Iteration[100/100] Loss: 0.5905 Acc:59.50%\nValid set Accuracy:60.00%\nTraining: Epoch[008/020] Iteration[010/100] Loss: 0.6696 Acc:57.50%\nTraining: Epoch[008/020] Iteration[020/100] Loss: 0.6346 Acc:60.00%\nTraining: Epoch[008/020] Iteration[030/100] Loss: 0.7112 Acc:59.17%\nTraining: Epoch[008/020] Iteration[040/100] Loss: 0.6375 Acc:61.25%\nTraining: Epoch[008/020] Iteration[050/100] Loss: 0.7218 Acc:61.00%\nTraining: Epoch[008/020] Iteration[060/100] Loss: 0.7120 Acc:59.58%\nTraining: Epoch[008/020] Iteration[070/100] Loss: 0.5738 Acc:59.64%\nTraining: Epoch[008/020] Iteration[080/100] Loss: 0.5056 Acc:60.62%\nTraining: Epoch[008/020] Iteration[090/100] Loss: 0.6651 Acc:59.44%\nTraining: Epoch[008/020] Iteration[100/100] Loss: 0.6838 Acc:59.50%\nTraining: Epoch[009/020] Iteration[010/100] Loss: 0.4123 Acc:70.00%\nTraining: Epoch[009/020] Iteration[020/100] Loss: 0.6963 Acc:65.00%\nTraining: Epoch[009/020] Iteration[030/100] Loss: 0.6396 Acc:60.00%\nTraining: Epoch[009/020] Iteration[040/100] Loss: 0.6160 Acc:60.62%\nTraining: Epoch[009/020] Iteration[050/100] Loss: 0.7060 Acc:60.50%\nTraining: Epoch[009/020] Iteration[060/100] Loss: 0.7648 Acc:59.58%\nTraining: Epoch[009/020] Iteration[070/100] Loss: 0.7516 Acc:59.29%\nTraining: Epoch[009/020] Iteration[080/100] Loss: 0.6546 Acc:58.44%\nTraining: Epoch[009/020] Iteration[090/100] Loss: 0.6162 Acc:59.72%\nTraining: Epoch[009/020] Iteration[100/100] Loss: 0.6484 Acc:59.75%\nValid set Accuracy:60.00%\nTraining: Epoch[010/020] Iteration[010/100] Loss: 0.6249 Acc:60.00%\nTraining: Epoch[010/020] Iteration[020/100] Loss: 0.6288 Acc:57.50%\nTraining: Epoch[010/020] Iteration[030/100] Loss: 0.5546 Acc:59.17%\nTraining: Epoch[010/020] Iteration[040/100] Loss: 0.6931 Acc:60.00%\nTraining: Epoch[010/020] Iteration[050/100] Loss: 0.5837 Acc:60.00%\nTraining: Epoch[010/020] Iteration[060/100] Loss: 0.5911 Acc:61.25%\nTraining: Epoch[010/020] Iteration[070/100] Loss: 0.6516 Acc:61.43%\nTraining: Epoch[010/020] Iteration[080/100] Loss: 0.6445 Acc:61.25%\nTraining: Epoch[010/020] Iteration[090/100] Loss: 0.7194 Acc:60.28%\nTraining: Epoch[010/020] Iteration[100/100] Loss: 0.8133 Acc:59.50%\nTraining: Epoch[011/020] Iteration[010/100] Loss: 0.6146 Acc:60.00%\nTraining: Epoch[011/020] Iteration[020/100] Loss: 0.6897 Acc:62.50%\nTraining: Epoch[011/020] Iteration[030/100] Loss: 0.7836 Acc:58.33%\nTraining: Epoch[011/020] Iteration[040/100] Loss: 0.6833 Acc:56.25%\nTraining: Epoch[011/020] Iteration[050/100] Loss: 0.5369 Acc:58.50%\nTraining: Epoch[011/020] Iteration[060/100] Loss: 0.7116 Acc:57.50%\nTraining: Epoch[011/020] Iteration[070/100] Loss: 0.6140 Acc:57.86%\nTraining: Epoch[011/020] Iteration[080/100] Loss: 0.6797 Acc:57.81%\nTraining: Epoch[011/020] Iteration[090/100] Loss: 0.5781 Acc:58.06%\nTraining: Epoch[011/020] Iteration[100/100] Loss: 0.6130 Acc:59.00%\nValid set Accuracy:60.00%\nTraining: Epoch[012/020] Iteration[010/100] Loss: 0.6476 Acc:60.00%\nTraining: Epoch[012/020] Iteration[020/100] Loss: 0.7374 Acc:58.75%\nTraining: Epoch[012/020] Iteration[030/100] Loss: 0.5591 Acc:62.50%\nTraining: Epoch[012/020] Iteration[040/100] Loss: 0.4497 Acc:63.12%\nTraining: Epoch[012/020] Iteration[050/100] Loss: 0.5776 Acc:64.00%\nTraining: Epoch[012/020] Iteration[060/100] Loss: 0.6732 Acc:62.50%\nTraining: Epoch[012/020] Iteration[070/100] Loss: 0.7171 Acc:61.79%\nTraining: Epoch[012/020] Iteration[080/100] Loss: 0.7500 Acc:59.69%\nTraining: Epoch[012/020] Iteration[090/100] Loss: 0.8280 Acc:58.89%\nTraining: Epoch[012/020] Iteration[100/100] Loss: 0.5586 Acc:60.00%\nTraining: Epoch[013/020] Iteration[010/100] Loss: 0.7373 Acc:55.00%\nTraining: Epoch[013/020] Iteration[020/100] Loss: 0.6362 Acc:58.75%\nTraining: Epoch[013/020] Iteration[030/100] Loss: 0.7131 Acc:55.00%\nTraining: Epoch[013/020] Iteration[040/100] Loss: 0.5566 Acc:58.75%\nTraining: Epoch[013/020] Iteration[050/100] Loss: 0.6137 Acc:60.00%\nTraining: Epoch[013/020] Iteration[060/100] Loss: 0.6009 Acc:62.08%\nTraining: Epoch[013/020] Iteration[070/100] Loss: 0.7388 Acc:62.14%\nTraining: Epoch[013/020] Iteration[080/100] Loss: 0.6440 Acc:61.56%\nTraining: Epoch[013/020] Iteration[090/100] Loss: 0.5341 Acc:61.94%\nTraining: Epoch[013/020] Iteration[100/100] Loss: 0.7290 Acc:60.00%\nValid set Accuracy:60.00%\nTraining: Epoch[014/020] Iteration[010/100] Loss: 0.6055 Acc:65.00%\nTraining: Epoch[014/020] Iteration[020/100] Loss: 0.6350 Acc:65.00%\nTraining: Epoch[014/020] Iteration[030/100] Loss: 0.6576 Acc:61.67%\nTraining: Epoch[014/020] Iteration[040/100] Loss: 0.6235 Acc:61.88%\nTraining: Epoch[014/020] Iteration[050/100] Loss: 0.6583 Acc:62.00%\nTraining: Epoch[014/020] Iteration[060/100] Loss: 0.7341 Acc:60.83%\nTraining: Epoch[014/020] Iteration[070/100] Loss: 0.4249 Acc:63.21%\nTraining: Epoch[014/020] Iteration[080/100] Loss: 0.6619 Acc:62.81%\nTraining: Epoch[014/020] Iteration[090/100] Loss: 0.7705 Acc:61.67%\nTraining: Epoch[014/020] Iteration[100/100] Loss: 0.7206 Acc:60.00%\nTraining: Epoch[015/020] Iteration[010/100] Loss: 0.7589 Acc:55.00%\nTraining: Epoch[015/020] Iteration[020/100] Loss: 0.5938 Acc:56.25%\nTraining: Epoch[015/020] Iteration[030/100] Loss: 0.5534 Acc:61.67%\nTraining: Epoch[015/020] Iteration[040/100] Loss: 0.6562 Acc:61.25%\nTraining: Epoch[015/020] Iteration[050/100] Loss: 0.6710 Acc:60.00%\nTraining: Epoch[015/020] Iteration[060/100] Loss: 0.7167 Acc:59.17%\nTraining: Epoch[015/020] Iteration[070/100] Loss: 0.6482 Acc:58.93%\nTraining: Epoch[015/020] Iteration[080/100] Loss: 0.5232 Acc:59.06%\nTraining: Epoch[015/020] Iteration[090/100] Loss: 0.6681 Acc:59.17%\nTraining: Epoch[015/020] Iteration[100/100] Loss: 0.7015 Acc:59.00%\nValid set Accuracy:57.00%\nTraining: Epoch[016/020] Iteration[010/100] Loss: 0.6368 Acc:55.00%\nTraining: Epoch[016/020] Iteration[020/100] Loss: 0.8506 Acc:52.50%\nTraining: Epoch[016/020] Iteration[030/100] Loss: 0.5666 Acc:55.83%\nTraining: Epoch[016/020] Iteration[040/100] Loss: 0.7091 Acc:56.88%\nTraining: Epoch[016/020] Iteration[050/100] Loss: 0.6697 Acc:57.50%\nTraining: Epoch[016/020] Iteration[060/100] Loss: 0.6514 Acc:56.67%\nTraining: Epoch[016/020] Iteration[070/100] Loss: 0.6351 Acc:57.50%\nTraining: Epoch[016/020] Iteration[080/100] Loss: 0.6913 Acc:57.81%\nTraining: Epoch[016/020] Iteration[090/100] Loss: 0.4504 Acc:59.72%\nTraining: Epoch[016/020] Iteration[100/100] Loss: 0.6256 Acc:59.75%\nTraining: Epoch[017/020] Iteration[010/100] Loss: 0.4946 Acc:67.50%\nTraining: Epoch[017/020] Iteration[020/100] Loss: 0.7181 Acc:58.75%\nTraining: Epoch[017/020] Iteration[030/100] Loss: 0.7278 Acc:58.33%\nTraining: Epoch[017/020] Iteration[040/100] Loss: 0.6208 Acc:60.62%\nTraining: Epoch[017/020] Iteration[050/100] Loss: 0.5990 Acc:61.00%\nTraining: Epoch[017/020] Iteration[060/100] Loss: 0.6345 Acc:62.50%\nTraining: Epoch[017/020] Iteration[070/100] Loss: 0.6929 Acc:60.00%\nTraining: Epoch[017/020] Iteration[080/100] Loss: 0.5781 Acc:60.31%\nTraining: Epoch[017/020] Iteration[090/100] Loss: 0.7668 Acc:59.44%\nTraining: Epoch[017/020] Iteration[100/100] Loss: 0.6483 Acc:59.50%\nValid set Accuracy:60.00%\nTraining: Epoch[018/020] Iteration[010/100] Loss: 0.4942 Acc:82.50%\nTraining: Epoch[018/020] Iteration[020/100] Loss: 0.7264 Acc:70.00%\nTraining: Epoch[018/020] Iteration[030/100] Loss: 0.5651 Acc:66.67%\nTraining: Epoch[018/020] Iteration[040/100] Loss: 0.7809 Acc:63.12%\nTraining: Epoch[018/020] Iteration[050/100] Loss: 0.5596 Acc:63.50%\nTraining: Epoch[018/020] Iteration[060/100] Loss: 0.6805 Acc:63.75%\nTraining: Epoch[018/020] Iteration[070/100] Loss: 0.6666 Acc:62.86%\nTraining: Epoch[018/020] Iteration[080/100] Loss: 0.6829 Acc:61.25%\nTraining: Epoch[018/020] Iteration[090/100] Loss: 0.6016 Acc:60.83%\nTraining: Epoch[018/020] Iteration[100/100] Loss: 0.7268 Acc:60.00%\nTraining: Epoch[019/020] Iteration[010/100] Loss: 0.7731 Acc:60.00%\nTraining: Epoch[019/020] Iteration[020/100] Loss: 0.4659 Acc:62.50%\nTraining: Epoch[019/020] Iteration[030/100] Loss: 0.6937 Acc:61.67%\nTraining: Epoch[019/020] Iteration[040/100] Loss: 0.6152 Acc:60.62%\nTraining: Epoch[019/020] Iteration[050/100] Loss: 0.6599 Acc:60.00%\nTraining: Epoch[019/020] Iteration[060/100] Loss: 0.5552 Acc:61.67%\nTraining: Epoch[019/020] Iteration[070/100] Loss: 0.6717 Acc:61.79%\nTraining: Epoch[019/020] Iteration[080/100] Loss: 0.5889 Acc:61.56%\nTraining: Epoch[019/020] Iteration[090/100] Loss: 0.6885 Acc:61.39%\nTraining: Epoch[019/020] Iteration[100/100] Loss: 0.7632 Acc:59.75%\nValid set Accuracy:60.00%\nTraining: Epoch[020/020] Iteration[010/100] Loss: 0.6698 Acc:57.50%\nTraining: Epoch[020/020] Iteration[020/100] Loss: 0.7038 Acc:55.00%\nTraining: Epoch[020/020] Iteration[030/100] Loss: 0.5916 Acc:56.67%\nTraining: Epoch[020/020] Iteration[040/100] Loss: 0.7645 Acc:58.75%\nTraining: Epoch[020/020] Iteration[050/100] Loss: 0.5945 Acc:59.50%\nTraining: Epoch[020/020] Iteration[060/100] Loss: 0.5590 Acc:60.83%\nTraining: Epoch[020/020] Iteration[070/100] Loss: 0.6311 Acc:60.36%\nTraining: Epoch[020/020] Iteration[080/100] Loss: 0.6971 Acc:59.69%\nTraining: Epoch[020/020] Iteration[090/100] Loss: 0.6469 Acc:59.72%\nTraining: Epoch[020/020] Iteration[100/100] Loss: 0.6143 Acc:59.50%\nFinished Training\n"
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "\n",
    "    loss_sigma = 0.0    # 记录一个epoch的loss之和\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # if i == 30 : break\n",
    "        # 获取图片和标签\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor(list(map(int, labels)))\n",
    "        labels = labels.to(device)\n",
    "        # inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward, backward, update weights\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计预测信息\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # correct += (predicted == labels).squeeze().sum().numpy()\n",
    "        correct += (predicted == labels).squeeze().sum().cpu().numpy()\n",
    "        loss_sigma += loss.item()\n",
    "\n",
    "        # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "        if i % 10 == 9:\n",
    "            loss_avg = loss_sigma / 10\n",
    "            loss_sigma = 0.0\n",
    "            print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch + 1, max_epoch, i + 1, len(train_loader), loss_avg, correct / total))\n",
    "\n",
    "            # # 记录训练loss\n",
    "            # writer.add_scalars('Loss_group', {'train_loss': loss_avg}, epoch)\n",
    "            # # 记录learning rate\n",
    "            # writer.add_scalar('learning rate', scheduler.get_lr()[0], epoch)\n",
    "            # # 记录Accuracy\n",
    "            # writer.add_scalars('Accuracy_group', {'train_acc': correct / total}, epoch)\n",
    "\n",
    "    # # 每个epoch，记录梯度，权值\n",
    "    # for name, layer in net.named_parameters():\n",
    "    #     writer.add_histogram(name + '_grad', layer.grad.cpu().data.numpy(), epoch)\n",
    "    #     writer.add_histogram(name + '_data', layer.cpu().data.numpy(), epoch)\n",
    "\n",
    "    # ------------------------------------ 观察模型在验证集上的表现 ------------------------------------\n",
    "    if epoch % 2 == 0:\n",
    "        loss_sigma = 0.0\n",
    "        cls_num = len(classes_name)\n",
    "        conf_mat = np.zeros([cls_num, cls_num])  # 混淆矩阵\n",
    "        net.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            # 获取图片和标签\n",
    "            # images, labels = data\n",
    "            # images, labels = Variable(images), Variable(labels)\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.tensor(list(map(int, labels)))\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "            outputs.detach_()\n",
    "\n",
    "            # 计算loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_sigma += loss.item()\n",
    "\n",
    "            # 统计\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # labels = labels.data    # Variable --> tensor\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                # cate_i = labels[j].numpy()\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                # pre_i = predicted[j].numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.0\n",
    "\n",
    "        print('{} set Accuracy:{:.2%}'.format('Valid', conf_mat.trace() / conf_mat.sum()))\n",
    "        # # 记录Loss, accuracy\n",
    "        # writer.add_scalars('Loss_group', {'valid_loss': loss_sigma / len(test_loader)}, epoch)\n",
    "        # writer.add_scalars('Accuracy_group', {'valid_acc': conf_mat.trace() / conf_mat.sum()}, epoch)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}